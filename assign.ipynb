{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('C:/Users/USER/Documents/dataset/lang.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text']=data[\"Text\"].str.lower()\n",
    "x=data['Text']\n",
    "y=data['language']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanLower(texts):\n",
    "    \n",
    "    # This regular expression pattern means everything except alphabetical characters\n",
    "    pattern = \"^[a-zA-Z]\"\n",
    "    cleanText = []\n",
    "    for text in texts:\n",
    "        # re.sub(pattern) means replace everything with a space except alphabetical characters\n",
    "        cleanText.append(re.sub(pattern,\" \",text).lower())\n",
    "        \n",
    "    return cleanText\n",
    "\n",
    "x = cleanLower(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ebes joseph pereira thomas  på eng the jesuits and the sino-russian treaty of nerchinsk  the diary of thomas pereira bibliotheca instituti historici s i --   rome libris '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Estonian\n",
       "1         Swedish\n",
       "2            Thai\n",
       "3           Tamil\n",
       "4           Dutch\n",
       "           ...   \n",
       "21995      French\n",
       "21996        Thai\n",
       "21997     Spanish\n",
       "21998     Chinese\n",
       "21999    Romanian\n",
       "Name: language, Length: 22000, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_words = 50000\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(list(data['Text']))\n",
    "train_data = tokenizer.texts_to_sequences(list(data['Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.keras.preprocessing.sequence.pad_sequences(train_data,maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,  2170,    82,  3638],\n",
       "       [    0,     0,     0, ...,    16,  4068, 26452],\n",
       "       [    0,     0,     0, ...,     0,     0, 16664],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,   118,  1435,   168],\n",
       "       [    0,     0,     0, ...,    16,  2029,   471],\n",
       "       [    0,     0,     0, ...,   178,  6523,     1]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arabic',\n",
       " 'Chinese',\n",
       " 'Dutch',\n",
       " 'English',\n",
       " 'Estonian',\n",
       " 'French',\n",
       " 'Hindi',\n",
       " 'Indonesian',\n",
       " 'Japanese',\n",
       " 'Korean',\n",
       " 'Latin',\n",
       " 'Persian',\n",
       " 'Portugese',\n",
       " 'Pushto',\n",
       " 'Romanian',\n",
       " 'Russian',\n",
       " 'Spanish',\n",
       " 'Swedish',\n",
       " 'Tamil',\n",
       " 'Thai',\n",
       " 'Turkish',\n",
       " 'Urdu']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2=le.fit_transform(y)\n",
    "total_languages = data['language'].nunique()\n",
    "y2 = np_utils.to_categorical(y2,num_classes=total_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(train_data,y2)\n",
    "\n",
    "embedding_dims = 500\n",
    "vocab_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(input_dim = vocab_size,output_dim = embedding_dims,input_length = max_len),\n",
    "                            tf.keras.layers.Flatten(),\n",
    "                            tf.keras.layers.Dense(total_languages,activation=tf.nn.softmax)\n",
    "                            ])\n",
    "\n",
    "\n",
    "model.compile(optimizer ='adam',loss = 'categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516/516 [==============================] - 1855s 4s/step - loss: 1.3418 - accuracy: 0.6449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1118095fcd0>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"エノが行きがかりでバスに乗ってしまい、気分が悪くなった際に助けるが、今すぐバスを降りたいと運転手に頼む際、本当のことを言ってしまうと彼女が恥ずかしい思いをすると察して「僕ウンコしたい\"]\n",
    "test_text = tokenizer.texts_to_sequences(text)\n",
    "test_text = tf.keras.preprocessing.sequence.pad_sequences(test_text, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predictions = model.predict(test_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chinese']\n"
     ]
    }
   ],
   "source": [
    "out = predictions.argmax()\n",
    "print(le.inverse_transform([out]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
